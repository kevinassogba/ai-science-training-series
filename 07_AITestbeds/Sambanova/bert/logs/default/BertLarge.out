Model:  BertLarge
Date:  11/27/24
Time:  00:50

WARNING: apt does not have a stable CLI interface. Use with caution in scripts.

Listing...
sambaflow/focal,now 1.19.1-44 amd64 [installed,upgradable to: 1.22.1-35]
Machine State Before: 
Platform: DataScale SN30-8

Physical Inventory:
Component Name                        | Serial Number       | Inventory State | Functional State
------------------------------------------------------------------------------------------------
/NODE/XRDU_0/RDU_0                    | 602852B16ABDB895    | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_0/DIMM_A0    | 1F6F59D             | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_1/DIMM_B0    | 1F6F804             | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_2/DIMM_E0    | 1F6F649             | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_3/DIMM_F0    | 1F6F803             | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_4/DIMM_G0    | 1F6F5AF             | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_5/DIMM_H0    | 1F6F805             | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_6/DIMM_C0    | 1F6F8A9             | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_7/DIMM_D0    | 1F6F606             | Present         | Online       
/NODE/XRDU_0/RDU_0/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1                    | 606052B16ABDB895    | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_0/DIMM_J0    | 1F6F69F             | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_1/DIMM_K0    | 1F6F6BF             | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_2/DIMM_N0    | 1F6F8AF             | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_3/DIMM_P0    | 1F6F621             | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_4/DIMM_Q0    | 1F6F655             | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_5/DIMM_R0    | 1F6F639             | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_6/DIMM_L0    | 1F6F5BC             | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_7/DIMM_M0    | 1F6F865             | Present         | Online       
/NODE/XRDU_0/RDU_1/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0                     | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_0              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_1              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_2              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_3              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_4              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_5              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_6              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_7              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_8              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_1                     | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_1/PORT_0              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_1/PORT_1              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_1/PORT_2              | N/A                 | Absent          | N/A          
/NODE/XRDU_0/SW_1/PORT_3              | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0                    | 4030707469B35895    | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_0/DIMM_A0    | 1F6F625             | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_1/DIMM_B0    | 1F6F5E9             | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_2/DIMM_E0    | 1F6F627             | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_3/DIMM_F0    | 1F6F8B9             | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_4/DIMM_G0    | 1F6F5A7             | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_5/DIMM_H0    | 1F6F8BB             | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_6/DIMM_C0    | 1F6F635             | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_7/DIMM_D0    | 1F6F8BA             | Present         | Online       
/NODE/XRDU_1/RDU_0/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1                    | 830387469B35895     | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_0/DIMM_J0    | 1F6F849             | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_1/DIMM_K0    | 1F6F8D8             | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_2/DIMM_N0    | 1F6F599             | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_3/DIMM_P0    | 1F6F8D9             | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_4/DIMM_Q0    | 1F6F84C             | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_5/DIMM_R0    | 1F6F7C5             | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_6/DIMM_L0    | 1F6F84B             | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_7/DIMM_M0    | 1F6F7BF             | Present         | Online       
/NODE/XRDU_1/RDU_1/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0                     | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_0              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_1              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_2              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_3              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_4              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_5              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_6              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_7              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_8              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_1                     | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_1/PORT_0              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_1/PORT_1              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_1/PORT_2              | N/A                 | Absent          | N/A          
/NODE/XRDU_1/SW_1/PORT_3              | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0                    | 2030787469B35895    | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_0/DIMM_A0    | 1F2E068             | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_1/DIMM_B0    | 1F2DFDC             | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_2/DIMM_E0    | 1F2DF4A             | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_3/DIMM_F0    | 1F2DF4B             | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_4/DIMM_G0    | 1F2DF85             | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_5/DIMM_H0    | 1F2DF23             | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_6/DIMM_C0    | 1F2E07B             | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_7/DIMM_D0    | 1F2E07D             | Present         | Online       
/NODE/XRDU_2/RDU_0/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1                    | 1048407469B35895    | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_0/DIMM_J0    | 1F5BAEB             | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_1/DIMM_K0    | 1F5BB52             | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_2/DIMM_N0    | 1F5BBAC             | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_3/DIMM_P0    | 1F5BD48             | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_4/DIMM_Q0    | 1F5BD71             | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_5/DIMM_R0    | 1F2DFD8             | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_6/DIMM_L0    | 1F5BD9C             | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_7/DIMM_M0    | 1F5BD34             | Present         | Online       
/NODE/XRDU_2/RDU_1/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0                     | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_0              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_1              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_2              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_3              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_4              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_5              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_6              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_7              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_8              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_1                     | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_1/PORT_0              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_1/PORT_1              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_1/PORT_2              | N/A                 | Absent          | N/A          
/NODE/XRDU_2/SW_1/PORT_3              | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0                    | 20301BB469B35895    | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_0/DIMM_A0    | 1F6CF78             | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_1/DIMM_B0    | 1F6D080             | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_2/DIMM_E0    | 1F6CF86             | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_3/DIMM_F0    | 1F6D117             | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_4/DIMM_G0    | 1F6CF60             | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_5/DIMM_H0    | 1F6D107             | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_6/DIMM_C0    | 1F6D092             | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_7/DIMM_D0    | 1F6D06E             | Present         | Online       
/NODE/XRDU_3/RDU_0/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1                    | 20702BB469B35895    | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_0/DIMM_J0    | 1F6D02D             | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_1/DIMM_K0    | 1F6D08A             | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_2/DIMM_N0    | 1F6CDCC             | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_3/DIMM_P0    | 1F6CF30             | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_4/DIMM_Q0    | 1F6CDE5             | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_5/DIMM_R0    | 1F6D11C             | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_6/DIMM_L0    | 1F6CDD0             | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_7/DIMM_M0    | 1F6CFF1             | Present         | Online       
/NODE/XRDU_3/RDU_1/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0                     | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_0              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_1              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_2              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_3              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_4              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_5              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_6              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_7              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_8              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_1                     | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_1/PORT_0              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_1/PORT_1              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_1/PORT_2              | N/A                 | Absent          | N/A          
/NODE/XRDU_3/SW_1/PORT_3              | N/A                 | Present         | Online       
/NODE/HOST/HIC_0/DPORT                | N/A                 | Present         | Online       
/NODE/HOST/HIC_1/DPORT                | N/A                 | Present         | Online       
/NODE/HOST/HIC_2/DPORT                | N/A                 | Present         | Online       
/NODE/HOST/HIC_3/DPORT                | N/A                 | Present         | Online
COMPILE START AT 0
COMPILE COMMAND: python /opt/sambaflow/apps/nlp/transformers_on_rdu/transformers_hook.py compile --model_name_or_path bert-large-uncased --tokenizer_name bert-large-uncased --module_name mlm_ns --task_name mlm_ns  --max_seq_length 128 --per_device_train_batch_size 256 -b 256 --output_dir=/home/keveltun/BertLarge/hf_output --overwrite_output_dir --cache_dir /home/keveltun/BertLarge/cache --compiler-configs-file /opt/sambaflow/apps/nlp/transformers_on_rdu/human_decisions/compiler_configs/compiler_configs_bertlarge_sc_mlm_ml_perf_fullfeature_macv2_gm.json --mac-human-decision /opt/sambaflow/apps/nlp/transformers_on_rdu/human_decisions/mac_overrides/bertlarge_sc_training_mlm_ml_perf_fullfeature_macv2.json --mac-v2 --non_split_head --dense_adam --data-parallel -ws 2 --weight_decay 0.01 --max_grad_norm_clip 1.0 --adam_beta2 0.98 --num-tiles 4 --pef-name=bertlrg --output-folder=/home/keveltun/BertLarge --log-level error --disable-strict-conversion
/opt/sambanova/lib/python3.8/site-packages/tensorflow/python/framework/dtypes.py:209: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)
  np.bool8: (False, True),
/opt/sambanova/lib/python3.8/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/opt/sambanova/lib/python3.8/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:326: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)
  np.bool8: (False, True),
/opt/sambanova/lib/python3.8/site-packages/modelbox/__init__.py:1: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.1.0/migration/
  from sambaflow import samba
/opt/sambanova/lib/python3.8/site-packages/pydantic/main.py:913: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.1.0/migration/
  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', DeprecationWarning)
/opt/sambaflow/apps/nlp/transformers_on_rdu/tasks/utils/patched_functions.py:2029: DeprecationWarning: invalid escape sequence \.
  tensor_names = ["attn\.c_attn\.weight", "attn\.c_proj\.weight", "mlp\.c_fc\.weight", "mlp\.c_proj\.weight"]
PyTorch version 2.0.1+cpu available.
TensorFlow version 2.7.0 available.
Downloading:   0%|          | 0.00/571 [00:00<?, ?B/s]Downloading: 100%|██████████| 571/571 [00:00<00:00, 830kB/s]
2024-11-27 00:50:55,633 - apps.__main__ - Rank 0, PID 1630107 - info     - NLP app started.
Downloading:   0%|          | 0.00/571 [00:00<?, ?B/s]Downloading: 100%|██████████| 571/571 [00:00<00:00, 1.08MB/s]
2024-11-27 00:50:55,741 - apps.tasks.utils.lazy_gpt2_pretrain - Rank 0, PID 1630107 - info     - Patching gpt2 hf model attr 'GPT2PreTrainedModel._init_weights'
2024-11-27 00:50:55,741 - apps.tasks.utils.lazy_gpt2_pretrain - Rank 0, PID 1630107 - info     - Patching mlm_ns hf model attr 'BertPreTrainedModel._init_weights'
Downloading:   0%|          | 0.00/1.34G [00:00<?, ?B/s]Downloading:   1%|          | 12.0M/1.34G [00:00<00:11, 120MB/s]Downloading:   2%|▏         | 24.9M/1.34G [00:00<00:10, 123MB/s]Downloading:   3%|▎         | 37.8M/1.34G [00:00<00:10, 124MB/s]Downloading:   4%|▍         | 50.6M/1.34G [00:00<00:10, 126MB/s]Downloading:   5%|▍         | 63.2M/1.34G [00:00<00:10, 119MB/s]Downloading:   6%|▌         | 77.0M/1.34G [00:00<00:10, 124MB/s]Downloading:   7%|▋         | 91.0M/1.34G [00:00<00:09, 128MB/s]Downloading:   8%|▊         | 105M/1.34G [00:00<00:09, 131MB/s] Downloading:   9%|▉         | 119M/1.34G [00:00<00:09, 133MB/s]Downloading:  10%|▉         | 133M/1.34G [00:01<00:08, 135MB/s]Downloading:  11%|█         | 147M/1.34G [00:01<00:08, 137MB/s]Downloading:  12%|█▏        | 161M/1.34G [00:01<00:08, 137MB/s]Downloading:  13%|█▎        | 175M/1.34G [00:01<00:08, 138MB/s]Downloading:  14%|█▍        | 188M/1.34G [00:01<00:08, 137MB/s]Downloading:  15%|█▌        | 202M/1.34G [00:01<00:08, 136MB/s]Downloading:  16%|█▌        | 216M/1.34G [00:01<00:08, 135MB/s]Downloading:  17%|█▋        | 229M/1.34G [00:01<00:08, 134MB/s]Downloading:  18%|█▊        | 242M/1.34G [00:01<00:08, 134MB/s]Downloading:  19%|█▉        | 256M/1.34G [00:01<00:08, 133MB/s]Downloading:  20%|██        | 269M/1.34G [00:02<00:08, 133MB/s]Downloading:  21%|██        | 282M/1.34G [00:02<00:08, 132MB/s]Downloading:  22%|██▏       | 296M/1.34G [00:02<00:07, 132MB/s]Downloading:  23%|██▎       | 309M/1.34G [00:02<00:07, 132MB/s]Downloading:  24%|██▍       | 322M/1.34G [00:02<00:07, 133MB/s]Downloading:  25%|██▍       | 336M/1.34G [00:02<00:07, 133MB/s]Downloading:  26%|██▌       | 349M/1.34G [00:02<00:07, 132MB/s]Downloading:  27%|██▋       | 362M/1.34G [00:02<00:07, 132MB/s]Downloading:  28%|██▊       | 375M/1.34G [00:02<00:07, 132MB/s]Downloading:  29%|██▉       | 388M/1.34G [00:02<00:07, 132MB/s]Downloading:  30%|██▉       | 402M/1.34G [00:03<00:07, 132MB/s]Downloading:  31%|███       | 415M/1.34G [00:03<00:07, 131MB/s]Downloading:  32%|███▏      | 428M/1.34G [00:03<00:06, 131MB/s]Downloading:  33%|███▎      | 441M/1.34G [00:03<00:06, 131MB/s]Downloading:  34%|███▍      | 454M/1.34G [00:03<00:06, 131MB/s]Downloading:  35%|███▍      | 468M/1.34G [00:03<00:06, 132MB/s]Downloading:  36%|███▌      | 481M/1.34G [00:03<00:06, 132MB/s]Downloading:  37%|███▋      | 494M/1.34G [00:03<00:06, 133MB/s]Downloading:  38%|███▊      | 507M/1.34G [00:03<00:06, 133MB/s]Downloading:  39%|███▊      | 521M/1.34G [00:03<00:06, 132MB/s]Downloading:  40%|███▉      | 534M/1.34G [00:04<00:06, 132MB/s]Downloading:  41%|████      | 547M/1.34G [00:04<00:06, 133MB/s]Downloading:  42%|████▏     | 561M/1.34G [00:04<00:05, 132MB/s]Downloading:  43%|████▎     | 574M/1.34G [00:04<00:05, 132MB/s]Downloading:  44%|████▎     | 587M/1.34G [00:04<00:05, 132MB/s]Downloading:  45%|████▍     | 600M/1.34G [00:04<00:05, 133MB/s]Downloading:  46%|████▌     | 614M/1.34G [00:04<00:05, 133MB/s]Downloading:  47%|████▋     | 627M/1.34G [00:04<00:05, 133MB/s]Downloading:  48%|████▊     | 640M/1.34G [00:04<00:05, 133MB/s]Downloading:  49%|████▊     | 654M/1.34G [00:04<00:05, 133MB/s]Downloading:  50%|████▉     | 667M/1.34G [00:05<00:05, 133MB/s]Downloading:  51%|█████     | 680M/1.34G [00:05<00:04, 133MB/s]Downloading:  52%|█████▏    | 694M/1.34G [00:05<00:04, 133MB/s]Downloading:  53%|█████▎    | 707M/1.34G [00:05<00:04, 133MB/s]Downloading:  54%|█████▎    | 720M/1.34G [00:05<00:04, 133MB/s]Downloading:  55%|█████▍    | 734M/1.34G [00:05<00:04, 132MB/s]Downloading:  56%|█████▌    | 747M/1.34G [00:05<00:04, 132MB/s]Downloading:  57%|█████▋    | 760M/1.34G [00:05<00:04, 132MB/s]Downloading:  58%|█████▊    | 773M/1.34G [00:05<00:04, 132MB/s]Downloading:  58%|█████▊    | 787M/1.34G [00:05<00:04, 132MB/s]Downloading:  59%|█████▉    | 800M/1.34G [00:06<00:04, 133MB/s]Downloading:  60%|██████    | 813M/1.34G [00:06<00:03, 133MB/s]Downloading:  61%|██████▏   | 827M/1.34G [00:06<00:03, 133MB/s]Downloading:  62%|██████▏   | 840M/1.34G [00:06<00:03, 134MB/s]Downloading:  63%|██████▎   | 854M/1.34G [00:06<00:03, 133MB/s]Downloading:  64%|██████▍   | 867M/1.34G [00:06<00:03, 134MB/s]Downloading:  65%|██████▌   | 880M/1.34G [00:06<00:03, 134MB/s]Downloading:  66%|██████▋   | 894M/1.34G [00:06<00:03, 134MB/s]Downloading:  67%|██████▋   | 907M/1.34G [00:06<00:03, 134MB/s]Downloading:  68%|██████▊   | 921M/1.34G [00:06<00:03, 130MB/s]Downloading:  69%|██████▉   | 934M/1.34G [00:07<00:03, 131MB/s]Downloading:  70%|███████   | 947M/1.34G [00:07<00:03, 131MB/s]Downloading:  71%|███████▏  | 960M/1.34G [00:07<00:02, 131MB/s]Downloading:  72%|███████▏  | 973M/1.34G [00:07<00:02, 131MB/s]Downloading:  73%|███████▎  | 986M/1.34G [00:07<00:02, 131MB/s]Downloading:  74%|███████▍  | 1.00G/1.34G [00:07<00:02, 131MB/s]Downloading:  75%|███████▌  | 1.01G/1.34G [00:07<00:02, 131MB/s]Downloading:  76%|███████▋  | 1.03G/1.34G [00:07<00:02, 130MB/s]Downloading:  77%|███████▋  | 1.04G/1.34G [00:07<00:02, 130MB/s]Downloading:  78%|███████▊  | 1.05G/1.34G [00:07<00:02, 129MB/s]Downloading:  79%|███████▉  | 1.06G/1.34G [00:08<00:02, 129MB/s]Downloading:  80%|████████  | 1.08G/1.34G [00:08<00:02, 129MB/s]Downloading:  81%|████████  | 1.09G/1.34G [00:08<00:02, 126MB/s]Downloading:  82%|████████▏ | 1.10G/1.34G [00:08<00:01, 125MB/s]Downloading:  83%|████████▎ | 1.12G/1.34G [00:08<00:01, 126MB/s]Downloading:  84%|████████▍ | 1.13G/1.34G [00:08<00:01, 127MB/s]Downloading:  85%|████████▍ | 1.14G/1.34G [00:08<00:01, 127MB/s]Downloading:  86%|████████▌ | 1.15G/1.34G [00:08<00:01, 128MB/s]Downloading:  87%|████████▋ | 1.17G/1.34G [00:08<00:01, 128MB/s]Downloading:  88%|████████▊ | 1.18G/1.34G [00:08<00:01, 128MB/s]Downloading:  89%|████████▊ | 1.19G/1.34G [00:09<00:01, 128MB/s]Downloading:  90%|████████▉ | 1.21G/1.34G [00:09<00:01, 125MB/s]Downloading:  91%|█████████ | 1.22G/1.34G [00:09<00:01, 126MB/s]Downloading:  92%|█████████▏| 1.23G/1.34G [00:09<00:00, 126MB/s]Downloading:  92%|█████████▏| 1.24G/1.34G [00:09<00:00, 127MB/s]Downloading:  93%|█████████▎| 1.26G/1.34G [00:09<00:00, 128MB/s]Downloading:  94%|█████████▍| 1.27G/1.34G [00:09<00:00, 128MB/s]Downloading:  95%|█████████▌| 1.28G/1.34G [00:09<00:00, 129MB/s]Downloading:  96%|█████████▋| 1.30G/1.34G [00:09<00:00, 129MB/s]Downloading:  97%|█████████▋| 1.31G/1.34G [00:09<00:00, 130MB/s]Downloading:  98%|█████████▊| 1.32G/1.34G [00:10<00:00, 130MB/s]Downloading:  99%|█████████▉| 1.33G/1.34G [00:10<00:00, 129MB/s]Downloading: 100%|██████████| 1.34G/1.34G [00:10<00:00, 131MB/s]
2024-11-27 00:51:06,477 - py.warnings - Rank 0, PID 1630107 - warning  - /opt/sambanova/lib/python3.8/site-packages/torch/nn/modules/module.py:572: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at /scratch/jobs/31685244/debbuild/common/sambanova-deps-pytorch-2.0.1-1/torch/csrc/utils/python_arg_parser.cpp:363.)
  elif param.grad_fn:

Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['cls.predictions.decoder.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-11-27 00:51:09,123 - apps.tasks.utils.lazy_gpt2_pretrain - Rank 0, PID 1630107 - warning  - This module is not supported under lazy param initialization: tasks.lm_tasks.bert_mlperf_lm.BertForMaskedLM.BertLMPredictionHead
Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]Downloading: 100%|██████████| 232k/232k [00:00<00:00, 6.66MB/s]
Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]Downloading: 100%|██████████| 466k/466k [00:00<00:00, 32.5MB/s]
Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]Downloading: 100%|██████████| 48.0/48.0 [00:00<00:00, 92.1kB/s]
2024-11-27 00:51:11,772 - py.warnings - Rank 0, PID 1630107 - warning  - /opt/sambanova/lib/python3.8/site-packages/torch/optim/optimizer.py:514: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at /scratch/jobs/31685244/debbuild/common/sambanova-deps-pytorch-2.0.1-1/torch/csrc/utils/python_arg_parser.cpp:363.)
  if not self.defaults.get('differentiable', None) and not (param.is_leaf or param.retains_grad):

2024-11-27 00:51:11,800 - apps.__main__ - Rank 0, PID 1630107 - info     - Dataset is loading.
2024-11-27 00:51:11,800 - apps.__main__ - Rank 0, PID 1630107 - info     - Dataset has finished loading.
2024-11-27 00:51:11,801 - apps.__main__ - Rank 0, PID 1630107 - info     - transformers_hook app running in compile mode
Log ID initialized to: [keveltun][python][1630107] at /var/log/sambaflow/runtime/sn.log
2024-11-27 00:51:12,278 - py.warnings - Rank 0, PID 1630107 - warning  - /opt/sambanova/lib/python3.8/site-packages/torch/overrides.py:1545: DeprecationWarning: Defining your `__torch_function__ as a plain method is deprecated and will be an error in future, please define it as a classmethod.
  warnings.warn("Defining your `__torch_function__ as a plain method is deprecated and "

2024-11-27 00:51:12,281 - py.warnings - Rank 0, PID 1630107 - warning  - /opt/sambanova/lib/python3.8/site-packages/torch/nn/modules/linear.py:114: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at /scratch/jobs/31685244/debbuild/common/sambanova-deps-pytorch-2.0.1-1/torch/csrc/utils/python_arg_parser.cpp:363.)
  return F.linear(input, self.weight, self.bias)

2024-11-27 00:51:12,283 - py.warnings - Rank 0, PID 1630107 - warning  - /opt/sambanova/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:291: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at /scratch/jobs/31685244/debbuild/common/sambanova-deps-pytorch-2.0.1-1/torch/csrc/utils/python_arg_parser.cpp:363.)
  attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))

2024-11-27 00:51:12,421 - py.warnings - Rank 0, PID 1630107 - warning  - /opt/sambanova/lib/python3.8/site-packages/torch/nn/modules/activation.py:359: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at /scratch/jobs/31685244/debbuild/common/sambanova-deps-pytorch-2.0.1-1/torch/csrc/utils/python_arg_parser.cpp:363.)
  return torch.tanh(input)

[info    ]: section boundary name: fwd_section_boundaries
[info    ]: section type 1: 
[info    ]: section boundary name: bwd_section_boundaries
[info    ]: section type 1: 
[info    ]: section boundary name: opt_section_boundaries
[info    ]: section type 1: 
[info    ]: section boundary name: fwd_naming_boundaries
[info    ]: section type 1: 
[info    ]: section boundary name: bwd_naming_boundaries
[info    ]: section type 1: 
[info    ]: Using legacy setting, GraphAMP pass will not run.
[info    ]: Using legacy setting, GraphAMP Legalizer pass will not run.
[warning ]: Cannot get AIRToTLIRInfo for op: module_terminator
[warning ]: Cannot get AIRToTLIRInfo for op: module @partitions  {
}
[warning ]: Cannot get AIRToTLIRInfo for op: module_terminator
[warning ]: Cannot get AIRToTLIRInfo for op: module @preface  {
}
[warning ]: Cannot get AIRToTLIRInfo for op: module_terminator
[warning ]: Cannot get AIRToTLIRInfo for op: module @templates  {
}
[warning ]: DimensionMapping for %1615 = "air.Embedding"(%0, %1) {air.kEstimateToleranceLatency = -1.000000e+00 : f64, air.kEstimateToleranceUtilization = -1.000000e+00 : f64, air.kGraphAmpDeny = false, air.kWeightGroupID = -1 : i64, kConfigured = true, kDoConvTiling = false, kDoRecompute = false, kInputsNamedDims = [["bertformaskedlm__bert__embeddings__token_type_embeddings__embedding_weight_dim0", "bertformaskedlm__bert__embeddings__word_embeddings__embedding_weight_dim1"], ["next_sentence_label_dim_0", "input_ids_dim_1"]], kInternalAddressesSlicing = false, kIsBubbledRecomputeNode = false, kMacConsumerNames = [["bertformaskedlm__bert__embeddings__add_t_input1"]], kNodeCategory = 1 : i64, kOutputsNamedDims = [["next_sentence_label_dim_0", "input_ids_dim_1", "bertformaskedlm__bert__embeddings__word_embeddings__embedding_weight_dim1"]]} : (tensor<2x1024xbf16>, tensor<256x128xi32>) -> tensor<256x128x1024xbf16> is not supported!
[info    ]: Building size named dims for graph hf_transformer_samba
[warning ]: Skip the process of updating air op's inputs/outputs size named dims.
[info    ]: Building nameddims for graph hf_transformer_samba
[warning ]: Skip the process of updating air op's inputs/outputs named dims.
[info    ]: Making Tensor Parallel Decisions for graph: hf_transformer_samba
[info    ]: Analyze tiling for graph: hf_transformer_samba
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__bert__pooler__dense__linear_t_input0
 inputs: {NamedTensor: shape {256, 1024} names: {next_sentence_label_dim_0, bertformaskedlm__bert__pooler__dense__weight_dim_1}}
 outputs: {NamedTensor: shape {1024, 256} names: {bertformaskedlm__bert__pooler__dense__weight_dim_1, next_sentence_label_dim_0}}
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__bert__pooler__dense__linear
 inputs: {NamedTensor: shape {1024, 1024} names: {bertformaskedlm__cls__seq_relationship__weight_dim_1, bertformaskedlm__bert__pooler__dense__linear_input_1_dim_1}, NamedTensor: shape {1024, 256} names: {bertformaskedlm__bert__pooler__dense__linear_input_1_dim_1, next_sentence_label_dim_0}, NamedTensor: shape {1024, 1} names: {bertformaskedlm__cls__seq_relationship__weight_dim_1, next_sentence_label_dim_0}}
 outputs: {NamedTensor: shape {1024, 256} names: {bertformaskedlm__cls__seq_relationship__weight_dim_1, next_sentence_label_dim_0}}
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__bert__pooler__activation__tanh
 inputs: {NamedTensor: shape {1024, 256} names: {bertformaskedlm__cls__seq_relationship__weight_dim_1, next_sentence_label_dim_0}}
 outputs: {NamedTensor: shape {1024, 256} names: {bertformaskedlm__cls__seq_relationship__weight_dim_1, next_sentence_label_dim_0}}
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__cls__seq_relationship__linear
 inputs: {NamedTensor: shape {2, 1024} names: {bertformaskedlm__cls__seq_relationship__weight_dim_0, bertformaskedlm__cls__seq_relationship__linear_input_1_dim_1}, NamedTensor: shape {1024, 256} names: {bertformaskedlm__cls__seq_relationship__linear_input_1_dim_1, next_sentence_label_dim_0}, NamedTensor: shape {2, 1} names: {bertformaskedlm__cls__seq_relationship__weight_dim_0, next_sentence_label_dim_0}}
 outputs: {NamedTensor: shape {2, 256} names: {bertformaskedlm__cls__seq_relationship__weight_dim_0, next_sentence_label_dim_0}}
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__cls__seq_relationship__linear_t_output0
 inputs: {NamedTensor: shape {2, 256} names: {bertformaskedlm__cls__seq_relationship__weight_dim_0, next_sentence_label_dim_0}}
 outputs: {NamedTensor: shape {256, 2} names: {next_sentence_label_dim_0, bertformaskedlm__cls__seq_relationship__weight_dim_0}}
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__crossentropyloss_1
 inputs: {NamedTensor: shape {256, 2} names: {next_sentence_label_dim_0, bertformaskedlm__crossentropyloss_1_num_classes}, NamedTensor: shape {256} names: {next_sentence_label_dim_0}}
 outputs: {NamedTensor: shape {256} names: {next_sentence_label_dim_0}}
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__crossentropyloss_swap_reshape
 inputs: {NamedTensor: shape {256, 128} names: {next_sentence_label_dim_0, labels_dim_1}}
 outputs: {NamedTensor: shape {32768} names: {bertformaskedlm__crossentropyloss_swap_reshape_output_dim_0}}
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__crossentropyloss_swap_reshape_bwd
 inputs: {NamedTensor: shape {32768} names: {bertformaskedlm__crossentropyloss_swap_reshape_output_dim_0}}
 outputs: {NamedTensor: shape {256, 128} names: {next_sentence_label_dim_0, labels_dim_1}}
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__bert__pooler__dense__linear_t_input0_bwd
 inputs: {NamedTensor: shape {1024, 256} names: {bertformaskedlm__bert__pooler__dense__weight_dim_1, next_sentence_label_dim_0}}
 outputs: {NamedTensor: shape {256, 1024} names: {next_sentence_label_dim_0, bertformaskedlm__bert__pooler__dense__weight_dim_1}}
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__bert__pooler__dense__linear_bwd_loss
 inputs: {NamedTensor: shape {1024, 1024} names: {bertformaskedlm__cls__seq_relationship__weight_dim_1, bertformaskedlm__bert__pooler__dense__linear_input_1_dim_1}, NamedTensor: shape {1024, 256} names: {bertformaskedlm__cls__seq_relationship__weight_dim_1, next_sentence_label_dim_0}}
 outputs: {NamedTensor: shape {1024, 256} names: {bertformaskedlm__bert__pooler__dense__linear_input_1_dim_1, next_sentence_label_dim_0}}
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__bert__pooler__activation__tanh_bwd
 inputs: {NamedTensor: shape {1024, 256} names: {bertformaskedlm__cls__seq_relationship__weight_dim_1, next_sentence_label_dim_0}, NamedTensor: shape {1024, 256} names: {bertformaskedlm__cls__seq_relationship__weight_dim_1, next_sentence_label_dim_0}}
 outputs: {NamedTensor: shape {1024, 256} names: {bertformaskedlm__cls__seq_relationship__weight_dim_1, next_sentence_label_dim_0}}
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__cls__seq_relationship__linear_bwd_loss
 inputs: {NamedTensor: shape {2, 1024} names: {bertformaskedlm__cls__seq_relationship__weight_dim_0, bertformaskedlm__cls__seq_relationship__linear_input_1_dim_1}, NamedTensor: shape {2, 256} names: {bertformaskedlm__cls__seq_relationship__weight_dim_0, next_sentence_label_dim_0}}
 outputs: {NamedTensor: shape {1024, 256} names: {bertformaskedlm__cls__seq_relationship__linear_input_1_dim_1, next_sentence_label_dim_0}}
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__cls__seq_relationship__linear_t_output0_bwd
 inputs: {NamedTensor: shape {256, 2} names: {next_sentence_label_dim_0, bertformaskedlm__cls__seq_relationship__weight_dim_0}}
 outputs: {NamedTensor: shape {2, 256} names: {bertformaskedlm__cls__seq_relationship__weight_dim_0, next_sentence_label_dim_0}}
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__crossentropyloss_1_bwd_loss
 inputs: {NamedTensor: shape {256, 2} names: {next_sentence_label_dim_0, bertformaskedlm__crossentropyloss_1_num_classes}, NamedTensor: shape {256} names: {next_sentence_label_dim_0}, NamedTensor: shape {256} names: {next_sentence_label_dim_0}}
 outputs: {NamedTensor: shape {256, 2} names: {next_sentence_label_dim_0, bertformaskedlm__crossentropyloss_1_num_classes}}
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__cls__seq_relationship__linear_bwd_weight
 inputs: {NamedTensor: shape {1024, 256} names: {bertformaskedlm__cls__seq_relationship__linear_input_1_dim_1, next_sentence_label_dim_0}, NamedTensor: shape {2, 256} names: {bertformaskedlm__cls__seq_relationship__weight_dim_0, next_sentence_label_dim_0}}
 outputs: {NamedTensor: shape {2, 1024} names: {bertformaskedlm__cls__seq_relationship__weight_dim_0, bertformaskedlm__cls__seq_relationship__linear_input_1_dim_1}, NamedTensor: shape {2, 1} names: {bertformaskedlm__cls__seq_relationship__weight_dim_0, bertformaskedlm__cls__seq_relationship__linear_bwd_weight_bias_output_size_1_dim}}
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__bert__pooler__dense__linear_bwd_weight
 inputs: {NamedTensor: shape {1024, 256} names: {bertformaskedlm__bert__pooler__dense__linear_input_1_dim_1, next_sentence_label_dim_0}, NamedTensor: shape {1024, 256} names: {bertformaskedlm__cls__seq_relationship__weight_dim_1, next_sentence_label_dim_0}}
 outputs: {NamedTensor: shape {1024, 1024} names: {bertformaskedlm__cls__seq_relationship__weight_dim_1, bertformaskedlm__bert__pooler__dense__linear_input_1_dim_1}, NamedTensor: shape {1024, 1} names: {bertformaskedlm__cls__seq_relationship__weight_dim_1, bertformaskedlm__bert__pooler__dense__linear_bwd_weight_bias_output_size_1_dim}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__0__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__0__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__0__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__0__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, input_ids_dim_1}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__0__attention__self__query__weight_dim_0, input_ids_dim_1}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__0__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__0__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__0__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__0__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, input_ids_dim_1}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__0__attention__self__key__weight_dim_0, input_ids_dim_1}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__0__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__0__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__0__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__0__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, input_ids_dim_1}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__0__attention__self__value__weight_dim_0, input_ids_dim_1}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__1__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__1__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__1__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__1__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__0__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__1__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__0__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__1__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__1__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__1__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__1__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__0__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__1__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__0__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__1__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__1__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__1__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__1__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__0__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__1__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__0__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__2__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__2__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__2__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__2__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__1__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__2__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__1__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__2__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__2__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__2__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__2__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__1__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__2__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__1__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__2__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__2__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__2__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__2__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__1__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__2__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__1__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__3__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__3__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__3__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__3__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__2__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__3__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__2__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__3__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__3__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__3__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__3__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__2__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__3__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__2__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__3__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__3__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__3__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__3__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__2__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__3__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__2__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__4__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__4__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__4__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__4__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__3__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__4__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__3__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__4__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__4__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__4__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__4__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__3__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__4__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__3__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__4__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__4__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__4__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__4__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__3__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__4__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__3__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__5__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__5__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__5__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__5__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__4__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__5__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__4__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__5__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__5__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__5__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__5__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__4__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__5__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__4__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__5__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__5__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__5__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__5__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__4__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__5__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__4__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__6__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__6__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__6__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__6__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__5__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__6__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__5__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__6__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__6__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__6__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__6__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__5__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__6__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__5__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__6__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__6__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__6__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__6__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__5__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__6__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__5__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__7__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__7__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__7__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__7__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__6__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__7__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__6__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__7__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__7__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__7__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__7__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__6__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__7__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__6__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__7__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__7__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__7__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__7__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__6__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__7__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__6__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__8__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__8__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__8__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__8__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__7__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__8__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__7__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__8__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__8__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__8__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__8__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__7__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__8__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__7__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__8__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__8__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__8__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__8__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__7__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__8__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__7__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__9__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__9__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__9__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__9__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__8__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__9__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__8__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__9__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__9__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__9__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__9__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__8__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__9__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__8__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__9__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__9__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__9__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__9__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__8__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__9__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__8__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__10__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__10__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__10__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__10__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__9__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__10__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__9__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__10__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__10__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__10__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__10__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__9__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__10__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__9__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__10__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__10__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__10__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__10__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__9__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__10__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__9__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__11__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__11__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__11__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__11__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__10__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__11__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__10__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__11__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__11__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__11__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__11__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__10__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__11__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__10__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__11__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__11__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__11__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__11__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__10__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__11__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__10__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__12__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__12__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__12__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__12__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__11__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__12__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__11__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__12__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__12__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__12__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__12__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__11__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__12__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__11__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__12__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__12__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__12__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__12__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__11__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__12__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__11__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__13__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__13__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__13__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__13__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__12__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__13__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__12__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__13__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__13__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__13__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__13__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__12__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__13__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__12__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__13__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__13__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__13__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__13__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__12__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__13__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__12__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__14__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__14__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__14__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__14__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__13__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__14__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__13__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__14__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__14__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__14__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__14__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__13__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__14__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__13__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__14__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__14__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__14__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__14__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__13__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__14__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__13__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__15__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__15__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__15__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__15__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__14__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__15__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__14__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__15__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__15__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__15__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__15__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__14__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__15__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__14__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__15__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__15__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__15__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__15__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__14__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__15__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__14__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__16__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__16__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__16__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__16__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__15__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__16__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__15__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__16__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__16__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__16__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__16__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__15__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__16__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__15__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__16__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__16__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__16__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__16__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__15__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__16__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__15__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__17__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__17__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__17__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__17__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__16__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__17__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__16__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__17__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__17__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__17__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__17__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__16__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__17__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__16__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__17__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__17__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__17__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__17__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__16__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__17__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__16__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__18__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__18__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__18__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__18__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__17__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__18__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__17__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__18__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__18__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__18__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__18__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__17__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__18__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__17__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__18__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__18__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__18__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__18__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__17__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__18__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__17__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__19__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__19__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__19__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__19__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__18__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__19__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__18__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__19__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__19__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__19__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__19__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__18__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__19__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__18__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__19__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__19__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__19__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__19__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__18__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__19__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__18__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__20__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__20__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__20__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__20__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__19__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__20__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__19__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__20__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__20__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__20__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__20__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__19__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__20__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__19__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__20__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__20__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__20__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__20__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__19__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__20__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__19__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__21__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__21__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__21__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__21__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__20__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__21__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__20__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__21__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__21__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__21__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__21__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__20__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__21__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__20__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__21__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__21__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__21__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__21__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__20__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__21__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__20__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__22__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__22__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__22__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__22__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__21__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__22__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__21__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__22__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__22__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__22__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__22__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__21__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__22__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__21__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__22__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__22__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__22__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__22__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__21__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__22__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__21__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__23__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__23__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__23__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__23__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__22__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__23__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__22__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__23__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__23__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__23__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__23__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__22__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__23__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__22__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__23__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__23__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__23__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__23__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__22__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__23__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__22__attention__self__matmul_dim_row}}
[info    ]: Mapping for graph/function: hf_transformer_samba
[info    ]: Amortized resources overall: 4.490000e+02 PCUs, 7.020000e+02 PMUs, projected latency: 6.059891e-01 s, FLOPS: 1.220349e+02 T/s, DDR_BW: 6.484095e+01 GB/s
[info    ]: Legalizing node resources...
[warning ]: [LinearInputGradInfo] STOC Rounding enabled but the selected template does not support STOC Rounding
[warning ]: [LinearInputGradInfo] STOC Rounding enabled but the selected template does not support STOC Rounding
[warning ]: [LinearInputGradInfo] STOC Rounding enabled but the selected template does not support STOC Rounding
[info    ]: Lowering to TLIR succeeded.
[info    ]: Not in hypersection mode. Can't convert/revert hyperpartitions
[warning ]: Skipped checking for identical layout in weight region and grad regions of Adam optimizer.
[info    ]: Compilation succeeded.
[info    ]: Mac Compilation succeeded.
2024-11-27 01:01:09,132 - apps.__main__ - Rank 0, PID 1630107 - info     - NLP app finished
import blocksparse tasks from _NamespacePath(['/opt/sambaflow/apps/nlp/transformers_on_rdu/blocksparse/common/tasks'])
import blocksparse tasks from _NamespacePath(['/opt/sambaflow/apps/nlp/transformers_on_rdu/blocksparse/common/tasks'])
+ /opt/sambanova/llvm16/bin/clang++ -DAT_PARALLEL_OPENMP=1 -D_THP_CORE -I/opt/llvm12/include -I./ -I/opt/sambaflow/include -fvisibility=hidden -fno-exceptions -fno-rtti -Werror -Wall -Wno-reorder -Wno-conversion -Wno-sign-conversion -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-variable -Wno-strict-aliasing -Wno-unused-local-typedefs -Wunreachable-code-aggressive -Wsometimes-uninitialized -Wconditional-uninitialized -Woverflow -Waddress-of-temporary -Wpointer-arith -Wimplicit -Wno-deprecated-declarations -Wbitfield-enum-conversion -Wno-unused-private-field -Wno-unused-but-set-variable -DSOURCE_PREFIX_LENGTH=84 -g -D_GNU_SOURCE -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -D__STDC_LIMIT_MACROS -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-unknown-pragmas -Wno-missing-braces -fopenmp=libomp -std=gnu++17 -o HfTransformerSamba.cpp.o -c HfTransformerSamba.cpp
+ /opt/sambanova/llvm16/bin/clang++ -DAT_PARALLEL_OPENMP=1 -D_THP_CORE -I/opt/llvm12/include -I./ -I/opt/sambaflow/include -fvisibility=hidden -fno-exceptions -fno-rtti -Werror -Wall -Wno-reorder -Wno-conversion -Wno-sign-conversion -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-variable -Wno-strict-aliasing -Wno-unused-local-typedefs -Wunreachable-code-aggressive -Wsometimes-uninitialized -Wconditional-uninitialized -Woverflow -Waddress-of-temporary -Wpointer-arith -Wimplicit -Wno-deprecated-declarations -Wbitfield-enum-conversion -Wno-unused-private-field -Wno-unused-but-set-variable -DSOURCE_PREFIX_LENGTH=84 -g -D_GNU_SOURCE -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -D__STDC_LIMIT_MACROS -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-unknown-pragmas -Wno-missing-braces -fopenmp=libomp -std=gnu++17 -o TestHfTransformerSamba.cpp.o -c TestHfTransformerSamba.cpp
+ /opt/sambanova/llvm16/bin/clang++ -I/opt/llvm12/include -I./ -I/opt/sambaflow/include -fvisibility=hidden -fno-exceptions -fno-rtti -Werror -Wall -Wno-reorder -Wno-conversion -Wno-sign-conversion -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-variable -Wno-strict-aliasing -Wno-unused-local-typedefs -Wunreachable-code-aggressive -Wsometimes-uninitialized -Wconditional-uninitialized -Woverflow -Waddress-of-temporary -Wpointer-arith -Wimplicit -Wno-deprecated-declarations -Wbitfield-enum-conversion -Wno-unused-private-field -Wno-unused-but-set-variable -DSOURCE_PREFIX_LENGTH=84 -g TestHfTransformerSamba.cpp.o HfTransformerSamba.cpp.o -o test_arc_bertlrg -L/opt/llvm12/lib -L/opt/sambaflow/lib -L/opt/sambaflow/lib64 -L/opt/sambanova/lib -L/opt/sambanova/lib64 -L/usr/local/lib -L/usr/local/lib64 -L/usr/lib/x86_64-linux-gnu -L/usr/lib64 -Wl,-rpath,/opt/llvm12/lib:/opt/sambaflow/lib:/opt/sambaflow/lib64:/opt/sambanova/lib:/opt/sambanova/lib64:/usr/local/lib:/usr/local/lib64:/usr/lib/x86_64-linux-gnu:/usr/lib64 -lLLVMSupport -lMLIRDialect -lMLIROptLib -lMLIRPass -lMLIRQuant -lMLIRStandard -lMLIRIR -lMLIRRewrite -lMLIRPDL -lMLIRPDLInterp -lMLIRTensor -lMLIRParser -lMLIRAnalysis -lMLIRTransforms -lMLIRTransformUtils -lMLIRLoopAnalysis -lMLIRControlFlowInterfaces -lMLIRSideEffectInterfaces -lMLIRViewLikeInterface -lMLIRAffine -lMLIRLoopLikeInterface -lMLIRSCF -lMLIRCallInterfaces -lMLIRPresburger -lMLIRCopyOpInterface -lMLIREDSC -lMLIRLinalg -lTemplates -lPrismPlasmaTemplates -lPrismPlasma -lPrismShared -lpef -lyaml-cpp -lCompilerShared -lMLIRDynamicWrapper -lRAIL -lRAILUtil -lisl -lTemplates -lMacCompiler -lArcCompiler -lArchSpec -lArchSpecCore -lomp -lpthread
+ set +x
[info    ]: Starting Plasma compile at /home/keveltun/BertLarge/bertlrg/plasma_ir_modules/schedule_global
[info    ]: [PASS] Running PlasmaIR000_PrinterPass
[info    ]: [PASS] Running PlasmaIR001_PlasmaCanonicalizer
[info    ]: [PASS] Running PlasmaIR002_SymbolChipAssignment
[info    ]: [PASS] Running PlasmaIR003_LegalizeLocationAssignment
[info    ]: [PASS] Running PlasmaIR004_SymbolAttributePropagation
[info    ]: [PASS] Running PlasmaIR005_ClusterOptimizerShardingSymbols
[info    ]: [PASS] Running PlasmaIR006_BuildDdrAndHostPools
[info    ]: [PASS] Running PlasmaIR007_DdrAndHostSymbolAssignment
[info    ]: [PASS] Running PlasmaIR008_SegmentGarbageCollection
[info    ]: [PASS] Running PlasmaIR009_DdrAndHostBitfileAssignment
[info    ]: [PASS] Running PlasmaIR010_PlasmaCanonicalizer
[info    ]: [PASS] Running PlasmaIR011_LegalizeDataAllocation
[info    ]: [PASS] Running PlasmaIR012_LegalizeSymbolOverlap
[info    ]: [PASS] Running PlasmaIR013_HbmLogical2Physical
[info    ]: [PASS] Running PlasmaIR014_PrinterPass
[info    ]: Plasma compile succeeded.
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_0_0_.
[info    ]: Compilation succeeded for partition_0_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_1_0_.
[info    ]: Compilation succeeded for partition_1_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_2_0_.
[info    ]: Compilation succeeded for partition_2_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_3_0_.
[info    ]: Compilation succeeded for partition_3_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_4_0_.
[info    ]: Compilation succeeded for partition_4_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_5_0_.
[info    ]: Compilation succeeded for partition_5_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_6_0_.
[info    ]: Compilation succeeded for partition_6_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_7_0_.
[warning ]: Cannot predict latency of context HfTransformerSamba.partition_7_0_.naming_group_25_bertformaskedlm__crossentropyloss_1@kInputXVec_0. Please suppress the warning by explicitly annotate the latency with set_ctx_latency() in the TBufferContext
../templates/src/templates/cross_entropy/rail/CrossEntropy.cpp:1535:0
tbuffer: partition_7_0_.naming_group_25_tbuf2a_7_0_35055 HfTransformerSamba.cpp:48383:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 1 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phase1_R_max phase2_R_exp, rd1: phase_rd_dummy, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_7_0_.HfTransformerSamba.partition_7_0_.naming_group_25_bertformaskedlm__crossentropyloss_1.naming_group_25_bertformaskedlm__crossentropyloss_1_tbuf_tmp ../templates/src/templates/cross_entropy/rail/CrossEntropy.cpp:2142:0
[info    ]: Compilation succeeded for partition_7_0_
[warning ]: Buffer instrumentation not supported for read contexts that have already a PmuDataSink
[warning ]: Buffer instrumentation not supported for read contexts that have already a PmuDataSink
[warning ]: Buffer instrumentation not supported for read contexts that have already a PmuDataSink
[warning ]: Buffer instrumentation not supported for read contexts that have already a PmuDataSink
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_8_0_.
[warning ]: You have created a transposed access but are not respecting slotting rules so there will be a performance drop!
../compiler/rail/src/lib/node/tbuffer/TBufferContext.cpp:349:0
ctx: kDefaultRead1 ../compiler/rail/src/lib/node/tbuffer/TBufferContext.cpp:349:0
pmu: D_0_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_8_0_.naming_group_26_tbuf2a_8_0_35164 HfTransformerSamba.cpp:48701:0
[warning ]: Injecting transpose slotting counter to context kDefaultRead1 of partition_8_0_.naming_group_26_tbuf1a_8_0_35532.D_0_0_0 to read one vector per 4 cycles!  The hardware requires this transposed read has address monotonically increasing by the vector width within windows of 4 vectors. The bandwidth penalty won't be incurred if rewriting the read context to run for a multiple of 4 cycles and use read predication to mask off unwanted vectors!
../compiler/rail/src/lib/node/tbuffer/TBufferContext.cpp:349:0
ctx: kDefaultRead1 ../compiler/rail/src/lib/node/tbuffer/TBufferContext.cpp:349:0
pmu: D_0_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_8_0_.naming_group_26_tbuf1a_8_0_35532 HfTransformerSamba.cpp:48717:0
[warning ]: You have created a transposed access but are not respecting slotting rules so there will be a performance drop!
../compiler/rail/src/lib/node/tbuffer/TBufferContext.cpp:349:0
ctx: kDefaultRead1 ../compiler/rail/src/lib/node/tbuffer/TBufferContext.cpp:349:0
pmu: D_0_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_8_0_.naming_group_26_tbuf1a_8_0_35532 HfTransformerSamba.cpp:48717:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_8_0_.HfTransformerSamba.partition_8_0_.naming_group_26_bertformaskedlm__crossentropyloss_1_bwd_loss.tbuf_tmp0 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[info    ]: Compilation succeeded for partition_8_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_9_0_.
[warning ]: Injecting transpose slotting counter to context kBackReadCtx of partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__cls__predictions__decoder__linear__gemm__0_bwd_weight_accum.tbuf_transpose_drain.D_0_0_0 to read one vector per 4 cycles!  The hardware requires this transposed read has address monotonically increasing by the vector width within windows of 4 vectors. The bandwidth penalty won't be incurred if rewriting the read context to run for a multiple of 4 cycles and use read predication to mask off unwanted vectors!
../templates/src/templates/accumulator/rail/ParAccum.cpp:1238:0
ctx: kBackReadCtx ../templates/src/templates/accumulator/rail/ParAccum.cpp:1238:0
pmu: D_0_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__cls__predictions__decoder__linear__gemm__0_bwd_weight_accum.tbuf_transpose_drain ../templates/src/templates/accumulator/rail/ParAccum.cpp:1231:0
[warning ]: You have created a transposed access but are not respecting slotting rules so there will be a performance drop!
../templates/src/templates/accumulator/rail/ParAccum.cpp:1238:0
ctx: kBackReadCtx ../templates/src/templates/accumulator/rail/ParAccum.cpp:1238:0
pmu: D_0_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__cls__predictions__decoder__linear__gemm__0_bwd_weight_accum.tbuf_transpose_drain ../templates/src/templates/accumulator/rail/ParAccum.cpp:1231:0
[warning ]: You have created a transposed access but are not respecting slotting rules so there will be a performance drop!
../templates/src/templates/accumulator/rail/ParAccum.cpp:1238:0
ctx: kBackReadCtx ../templates/src/templates/accumulator/rail/ParAccum.cpp:1238:0
pmu: D_0_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__cls__predictions__decoder__linear__gemm__1_bwd_weight_accum.tbuf_transpose_drain ../templates/src/templates/accumulator/rail/ParAccum.cpp:1231:0
[warning ]: You have created a transposed access but are not respecting slotting rules so there will be a performance drop!
../templates/src/templates/accumulator/rail/ParAccum.cpp:1238:0
ctx: kBackReadCtx ../templates/src/templates/accumulator/rail/ParAccum.cpp:1238:0
pmu: D_0_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__cls__predictions__decoder__linear__gemm__2_bwd_weight_accum.tbuf_transpose_drain ../templates/src/templates/accumulator/rail/ParAccum.cpp:1231:0
[warning ]: You have created a transposed access but are not respecting slotting rules so there will be a performance drop!
../templates/src/templates/accumulator/rail/ParAccum.cpp:1238:0
ctx: kBackReadCtx ../templates/src/templates/accumulator/rail/ParAccum.cpp:1238:0
pmu: D_0_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__cls__predictions__decoder__linear__gemm__3_bwd_weight_accum.tbuf_transpose_drain ../templates/src/templates/accumulator/rail/ParAccum.cpp:1231:0
[warning ]: Cannot predict latency of context kBackReadCtx. Please suppress the warning by explicitly annotate the latency with set_ctx_latency() in the TBufferContext
../templates/src/templates/accumulator/rail/ParAccum.cpp:1238:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__cls__predictions__decoder__linear__gemm__3_bwd_weight_accum.tbuf_transpose_drain ../templates/src/templates/accumulator/rail/ParAccum.cpp:1231:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__crossentropyloss_bwd_loss.tbuf_tmp0 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__crossentropyloss_bwd_loss.tbuf_tmp1 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__crossentropyloss_bwd_loss.tbuf_tmp2 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__crossentropyloss_bwd_loss.tbuf_tmp3 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__crossentropyloss_bwd_loss.tbuf_tmp4 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__crossentropyloss_bwd_loss.tbuf_tmp5 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__crossentropyloss_bwd_loss.tbuf_tmp6 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__crossentropyloss_bwd_loss.tbuf_tmp7 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__crossentropyloss_bwd_loss.tbuf_tmp8 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__crossentropyloss_bwd_loss.tbuf_tmp9 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__crossentropyloss_bwd_loss.tbuf_tmp10 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__crossentropyloss_bwd_loss.tbuf_tmp11 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__crossentropyloss_bwd_loss.tbuf_tmp12 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__crossentropyloss_bwd_loss.tbuf_tmp13 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__crossentropyloss_bwd_loss.tbuf_tmp14 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__crossentropyloss_bwd_loss.tbuf_tmp15 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__crossentropyloss_bwd_loss.tbuf_tmp16 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__crossentropyloss_bwd_loss.tbuf_tmp17 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__crossentropyloss_bwd_loss.tbuf_tmp18 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__crossentropyloss_bwd_loss.tbuf_tmp19 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__crossentropyloss_bwd_loss.tbuf_tmp20 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__crossentropyloss_bwd_loss.tbuf_tmp21 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__crossentropyloss_bwd_loss.tbuf_tmp22 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__crossentropyloss_bwd_loss.tbuf_tmp23 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__crossentropyloss_bwd_loss.tbuf_tmp24 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__crossentropyloss_bwd_loss.tbuf_tmp25 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__crossentropyloss_bwd_loss.tbuf_tmp26 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__crossentropyloss_bwd_loss.tbuf_tmp27 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__crossentropyloss_bwd_loss.tbuf_tmp28 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[warning ]: For read or write contexts operating on this PMU tensor, the number of read context-dones must equal the number of write context-dones to avoid hangs due implicit hardware UDC thinking that the scratchpad is full. For this PMU with depth: 1 and scratchpad tensor Tensor0, #rd0_dones: 2 (skip_rd0_ctx: 0), #rd1_dones: 1 (skip_rd1_ctx: 0), #wr0_dones: 2 (skip_wr0_ctx: 0), #wr1_dones: 1 (skip_wr1_ctx: 0). The read contexts on this port with missing read dones are: (rd0: phaseRExp0, rd1: phase_rd_dummy dummy_phaseRExp0_to_phaseRSumRecip_launcher, wr0: none, wr1: none). Check the usage of set_ctx_done() or set_done() on these contexts.
../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
pmu: D_0_0 ../compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:93:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__crossentropyloss_bwd_loss.tbuf_tmp29 ../templates/src/templates/cross_entropy/rail/CrossEntropyGrad.cpp:1813:0
[info    ]: Compilation succeeded for partition_9_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_10_0_.
[info    ]: Compilation succeeded for partition_10_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_11_0_.
[info    ]: Compilation succeeded for partition_11_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_12_0_.
[info    ]: Compilation succeeded for partition_12_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_13_0_.
[info    ]: Compilation succeeded for partition_13_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_14_0_.
[info    ]: Compilation succeeded for partition_14_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_15_0_.
[info    ]: Compilation succeeded for partition_15_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_16_0_.
[info    ]: Compilation succeeded for partition_16_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_17_0_.
[info    ]: Compilation succeeded for partition_17_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_18_0_.
[info    ]: Compilation succeeded for partition_18_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_19_0_.
[info    ]: Compilation succeeded for partition_19_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_20_0_.
[info    ]: Compilation succeeded for partition_20_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_21_0_.
[info    ]: Compilation succeeded for partition_21_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_22_0_.
[info    ]: Compilation succeeded for partition_22_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_23_0_.
[info    ]: Compilation succeeded for partition_23_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_24_0_.
[info    ]: Compilation succeeded for partition_24_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_25_0_.
[info    ]: Compilation succeeded for partition_25_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_26_0_.
[info    ]: Compilation succeeded for partition_26_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_27_0_.
[info    ]: Compilation succeeded for partition_27_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_28_0_.
[info    ]: Compilation succeeded for partition_28_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_29_0_.
[info    ]: Compilation succeeded for partition_29_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_30_0_.
[info    ]: Compilation succeeded for partition_30_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_31_0_.
[info    ]: Compilation succeeded for partition_31_0_
[info    ]: Starting Plasma compile at /home/keveltun/BertLarge/bertlrg/plasma_ir_modules/sections_0_31
[info    ]: [PASS] Running PlasmaIR000_PrinterPass
[info    ]: [PASS] Running PlasmaIR001_PlasmaCanonicalizer
[info    ]: [PASS] Running PlasmaIR002_PrinterPass
[info    ]: Plasma compile succeeded.
[info    ]: Starting Plasma compile at /home/keveltun/BertLarge/bertlrg/plasma_ir_modules/schedule_0_31
[info    ]: [PASS] Running PlasmaIR000_PrinterPass
[info    ]: [PASS] Running PlasmaIR001_PlasmaCanonicalizer
[info    ]: [PASS] Running PlasmaIR002_SymbolChipAssignment
[info    ]: [PASS] Running PlasmaIR003_LegalizeLocationAssignment
[info    ]: [PASS] Running PlasmaIR004_LegalizeIncrementalDataAllocation
[info    ]: [PASS] Running PlasmaIR005_DdrAndHostSymbolAssignment
[info    ]: [PASS] Running PlasmaIR006_PlasmaCanonicalizer
[info    ]: [PASS] Running PlasmaIR007_LegalizeDataAllocation
[info    ]: [PASS] Running PlasmaIR008_PrinterPass
[info    ]: Plasma compile succeeded.
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_32_0_.
[info    ]: Compilation succeeded for partition_32_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_33_0_.
[info    ]: Compilation succeeded for partition_33_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_34_0_.
[info    ]: Compilation succeeded for partition_34_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_35_0_.
[info    ]: Compilation succeeded for partition_35_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_36_0_.
[info    ]: Compilation succeeded for partition_36_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_37_0_.
[info    ]: Compilation succeeded for partition_37_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_38_0_.
[info    ]: Compilation succeeded for partition_38_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_39_0_.
[info    ]: Compilation succeeded for partition_39_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_40_0_.
[info    ]: Compilation succeeded for partition_40_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_41_0_.
[info    ]: Compilation succeeded for partition_41_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_42_0_.
[info    ]: Compilation succeeded for partition_42_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_43_0_.
[info    ]: Compilation succeeded for partition_43_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_44_0_.
[info    ]: Compilation succeeded for partition_44_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_45_0_.
[info    ]: Compilation succeeded for partition_45_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_46_0_.
[info    ]: Compilation succeeded for partition_46_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_47_0_.
[info    ]: Compilation succeeded for partition_47_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_48_0_.
[info    ]: Compilation succeeded for partition_48_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_49_0_.
[info    ]: Compilation succeeded for partition_49_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_50_0_.
[info    ]: Compilation succeeded for partition_50_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_51_0_.
[info    ]: Compilation succeeded for partition_51_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_52_0_.
[info    ]: Compilation succeeded for partition_52_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_53_0_.
[info    ]: Compilation succeeded for partition_53_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_54_0_.
[info    ]: Compilation succeeded for partition_54_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_55_0_.
[info    ]: Compilation succeeded for partition_55_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_56_0_.
[info    ]: Compilation succeeded for partition_56_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_57_0_.
[info    ]: Compilation succeeded for partition_57_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_58_0_.
[info    ]: Compilation succeeded for partition_58_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_59_0_.
[info    ]: Compilation succeeded for partition_59_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_60_0_.
[info    ]: Compilation succeeded for partition_60_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_61_0_.
[info    ]: Compilation succeeded for partition_61_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_62_0_.
[info    ]: Compilation succeeded for partition_62_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_63_0_.
[info    ]: Compilation succeeded for partition_63_0_
[info    ]: Starting Plasma compile at /home/keveltun/BertLarge/bertlrg/plasma_ir_modules/sections_32_63
[info    ]: [PASS] Running PlasmaIR000_PrinterPass
[info    ]: [PASS] Running PlasmaIR001_PlasmaCanonicalizer
[info    ]: [PASS] Running PlasmaIR002_PrinterPass
[info    ]: Plasma compile succeeded.
[info    ]: Starting Plasma compile at /home/keveltun/BertLarge/bertlrg/plasma_ir_modules/schedule_32_63
[info    ]: [PASS] Running PlasmaIR000_PrinterPass
[info    ]: [PASS] Running PlasmaIR001_PlasmaCanonicalizer
[info    ]: [PASS] Running PlasmaIR002_SymbolChipAssignment
[info    ]: [PASS] Running PlasmaIR003_LegalizeLocationAssignment
[info    ]: [PASS] Running PlasmaIR004_LegalizeIncrementalDataAllocation
[info    ]: [PASS] Running PlasmaIR005_DdrAndHostSymbolAssignment
[info    ]: [PASS] Running PlasmaIR006_PlasmaCanonicalizer
[info    ]: [PASS] Running PlasmaIR007_LegalizeDataAllocation
[info    ]: [PASS] Running PlasmaIR008_PrinterPass
[info    ]: Plasma compile succeeded.
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_64_0_.
[info    ]: Compilation succeeded for partition_64_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_65_0_.
[info    ]: Compilation succeeded for partition_65_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_66_0_.
[info    ]: Compilation succeeded for partition_66_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_67_0_.
[info    ]: Compilation succeeded for partition_67_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_68_0_.
[info    ]: Compilation succeeded for partition_68_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_69_0_.
[info    ]: Compilation succeeded for partition_69_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_70_0_.
[info    ]: Compilation succeeded for partition_70_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_71_0_.
[info    ]: Compilation succeeded for partition_71_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_72_0_.
[info    ]: Compilation succeeded for partition_72_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_73_0_.
[info    ]: Compilation succeeded for partition_73_0_
[info    ]: Logs are generated in /home/keveltun/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_74_0_.
[info    ]: Compilation succeeded for partition_74_0_
[info    ]: Starting Plasma compile at /home/keveltun/BertLarge/bertlrg/plasma_ir_modules/sections_64_74
[info    ]: [PASS] Running PlasmaIR000_PrinterPass
[info    ]: [PASS] Running PlasmaIR001_PlasmaCanonicalizer
[info    ]: [PASS] Running PlasmaIR002_PrinterPass
[info    ]: Plasma compile succeeded.
[info    ]: Starting Plasma compile at /home/keveltun/BertLarge/bertlrg/plasma_ir_modules/schedule_64_74
[info    ]: [PASS] Running PlasmaIR000_PrinterPass
[info    ]: [PASS] Running PlasmaIR001_PlasmaCanonicalizer
[info    ]: [PASS] Running PlasmaIR002_SymbolChipAssignment
[info    ]: [PASS] Running PlasmaIR003_LegalizeLocationAssignment
[info    ]: [PASS] Running PlasmaIR004_LegalizeIncrementalDataAllocation
[info    ]: [PASS] Running PlasmaIR005_DdrAndHostSymbolAssignment
[info    ]: [PASS] Running PlasmaIR006_PlasmaCanonicalizer
[info    ]: [PASS] Running PlasmaIR007_LegalizeDataAllocation
[info    ]: [PASS] Running PlasmaIR008_PrinterPass
[info    ]: Plasma compile succeeded.
[info    ]: Starting Plasma compile at /home/keveltun/BertLarge/bertlrg/plasma_ir_modules/schedule_final
[info    ]: [PASS] Running PlasmaIR000_PrinterPass
[info    ]: [PASS] Running PlasmaIR001_PlasmaCanonicalizer
[info    ]: [PASS] Running PlasmaIR002_PlasmaCanonicalizer
[info    ]: [PASS] Running PlasmaIR003_SegmentFinalization
[info    ]: [PASS] Running PlasmaIR004_LegalizeDataAllocation
[info    ]: [PASS] Running PlasmaIR005_LegalizeSymbolOverlap
[info    ]: [PASS] Running PlasmaIR006_PrinterPass
[info    ]: Plasma compile succeeded.
[info    ] : PEF file /home/keveltun/BertLarge/bertlrg//bertlrg.pef created
COMPILE END AT 624
RUN
SHELL=/bin/bash
PWD=/home/keveltun/BertLarge
LOGNAME=keveltun
OPENBLAS_NUM_THREADS=8
MOTD_SHOWN=pam
HOME=/home/keveltun
LANG=en_US.UTF-8
VIRTUAL_ENV=/opt/sambaflow/apps/nlp/transformers_on_rdu/venv
SSH_CONNECTION=140.221.69.48 37620 140.221.82.7 22
TERM=xterm-256color
LIBVIRT_DEFAULT_URI=qemu:///system
USER=keveltun
IBV_FORK_SAFE=1
SHLVL=2
SOFTWARE_HOME=/opt
PS1=(venv) 
SSH_CLIENT=140.221.69.48 37620 22
SN_NUM_THREADS=32
OMP_NUM_THREADS=18
XDG_DATA_DIRS=/usr/local/share:/usr/share:/var/lib/snapd/desktop
PATH=/opt/sambaflow/apps/nlp/transformers_on_rdu/venv/bin:/opt/sambanova/bin:/opt/sambaflow/bin:/opt/sambaflow/venv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/keveltun/.local/bin:/home/keveltun/bin
GLIBC_TUNABLES=glibc.cpu.hwcaps=-AVX_Usable,-AVX2_Usable,-Prefer_ERMS,-Prefer_FSRM,Prefer_No_AVX512,Prefer_No_VZEROUPPER,-AVX_Fast_Unaligned_Load,-ERMS
SSH_TTY=/dev/pts/2
OLDPWD=/home/keveltun
_=/usr/bin/env
Submitted batch job 44024
Machine state After: 
Platform: DataScale SN30-8

Physical Inventory:
Component Name                        | Serial Number       | Inventory State | Functional State
------------------------------------------------------------------------------------------------
/NODE/XRDU_0/RDU_0                    | 602852B16ABDB895    | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_0/DIMM_A0    | 1F6F59D             | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_1/DIMM_B0    | 1F6F804             | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_2/DIMM_E0    | 1F6F649             | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_3/DIMM_F0    | 1F6F803             | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_4/DIMM_G0    | 1F6F5AF             | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_5/DIMM_H0    | 1F6F805             | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_6/DIMM_C0    | 1F6F8A9             | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_7/DIMM_D0    | 1F6F606             | Present         | Online       
/NODE/XRDU_0/RDU_0/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1                    | 606052B16ABDB895    | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_0/DIMM_J0    | 1F6F69F             | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_1/DIMM_K0    | 1F6F6BF             | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_2/DIMM_N0    | 1F6F8AF             | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_3/DIMM_P0    | 1F6F621             | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_4/DIMM_Q0    | 1F6F655             | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_5/DIMM_R0    | 1F6F639             | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_6/DIMM_L0    | 1F6F5BC             | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_7/DIMM_M0    | 1F6F865             | Present         | Online       
/NODE/XRDU_0/RDU_1/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0                     | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_0              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_1              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_2              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_3              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_4              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_5              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_6              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_7              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_8              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_1                     | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_1/PORT_0              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_1/PORT_1              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_1/PORT_2              | N/A                 | Absent          | N/A          
/NODE/XRDU_0/SW_1/PORT_3              | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0                    | 4030707469B35895    | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_0/DIMM_A0    | 1F6F625             | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_1/DIMM_B0    | 1F6F5E9             | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_2/DIMM_E0    | 1F6F627             | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_3/DIMM_F0    | 1F6F8B9             | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_4/DIMM_G0    | 1F6F5A7             | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_5/DIMM_H0    | 1F6F8BB             | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_6/DIMM_C0    | 1F6F635             | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_7/DIMM_D0    | 1F6F8BA             | Present         | Online       
/NODE/XRDU_1/RDU_0/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1                    | 830387469B35895     | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_0/DIMM_J0    | 1F6F849             | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_1/DIMM_K0    | 1F6F8D8             | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_2/DIMM_N0    | 1F6F599             | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_3/DIMM_P0    | 1F6F8D9             | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_4/DIMM_Q0    | 1F6F84C             | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_5/DIMM_R0    | 1F6F7C5             | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_6/DIMM_L0    | 1F6F84B             | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_7/DIMM_M0    | 1F6F7BF             | Present         | Online       
/NODE/XRDU_1/RDU_1/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0                     | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_0              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_1              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_2              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_3              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_4              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_5              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_6              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_7              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_8              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_1                     | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_1/PORT_0              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_1/PORT_1              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_1/PORT_2              | N/A                 | Absent          | N/A          
/NODE/XRDU_1/SW_1/PORT_3              | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0                    | 2030787469B35895    | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_0/DIMM_A0    | 1F2E068             | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_1/DIMM_B0    | 1F2DFDC             | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_2/DIMM_E0    | 1F2DF4A             | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_3/DIMM_F0    | 1F2DF4B             | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_4/DIMM_G0    | 1F2DF85             | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_5/DIMM_H0    | 1F2DF23             | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_6/DIMM_C0    | 1F2E07B             | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_7/DIMM_D0    | 1F2E07D             | Present         | Online       
/NODE/XRDU_2/RDU_0/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1                    | 1048407469B35895    | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_0/DIMM_J0    | 1F5BAEB             | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_1/DIMM_K0    | 1F5BB52             | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_2/DIMM_N0    | 1F5BBAC             | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_3/DIMM_P0    | 1F5BD48             | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_4/DIMM_Q0    | 1F5BD71             | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_5/DIMM_R0    | 1F2DFD8             | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_6/DIMM_L0    | 1F5BD9C             | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_7/DIMM_M0    | 1F5BD34             | Present         | Online       
/NODE/XRDU_2/RDU_1/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0                     | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_0              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_1              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_2              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_3              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_4              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_5              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_6              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_7              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_8              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_1                     | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_1/PORT_0              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_1/PORT_1              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_1/PORT_2              | N/A                 | Absent          | N/A          
/NODE/XRDU_2/SW_1/PORT_3              | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0                    | 20301BB469B35895    | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_0/DIMM_A0    | 1F6CF78             | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_1/DIMM_B0    | 1F6D080             | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_2/DIMM_E0    | 1F6CF86             | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_3/DIMM_F0    | 1F6D117             | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_4/DIMM_G0    | 1F6CF60             | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_5/DIMM_H0    | 1F6D107             | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_6/DIMM_C0    | 1F6D092             | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_7/DIMM_D0    | 1F6D06E             | Present         | Online       
/NODE/XRDU_3/RDU_0/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1                    | 20702BB469B35895    | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_0/DIMM_J0    | 1F6D02D             | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_1/DIMM_K0    | 1F6D08A             | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_2/DIMM_N0    | 1F6CDCC             | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_3/DIMM_P0    | 1F6CF30             | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_4/DIMM_Q0    | 1F6CDE5             | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_5/DIMM_R0    | 1F6D11C             | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_6/DIMM_L0    | 1F6CDD0             | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_7/DIMM_M0    | 1F6CFF1             | Present         | Online       
/NODE/XRDU_3/RDU_1/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0                     | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_0              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_1              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_2              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_3              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_4              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_5              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_6              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_7              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_8              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_1                     | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_1/PORT_0              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_1/PORT_1              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_1/PORT_2              | N/A                 | Absent          | N/A          
/NODE/XRDU_3/SW_1/PORT_3              | N/A                 | Present         | Online       
/NODE/HOST/HIC_0/DPORT                | N/A                 | Present         | Online       
/NODE/HOST/HIC_1/DPORT                | N/A                 | Present         | Online       
/NODE/HOST/HIC_2/DPORT                | N/A                 | Present         | Online       
/NODE/HOST/HIC_3/DPORT                | N/A                 | Present         | Online
Duration:  624
